# Saturated Activation Detection

The goal is to add explicit detection for saturated activations. This will augment the existing [NeuralDbg](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#53-439) engine to better explain vanishing gradient failures.

## Proposed Changes

### NeuralDbg Engine

#### [MODIFY] [neuraldbg.py](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py)
- Update [_compute_activation_stats](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#187-212) to include a `saturation_ratio`.
- Update [_detect_activation_shift](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#213-227) to consider `saturation_ratio` changes.
- Add a specific method `_explain_saturated_activations` to provide hypotheses.
- Integrate this new explanation into [explain_failure](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#259-281).

## Verification Plan

### Automated Tests
I will create a new test file `tests/repro_saturation.py` that uses a Tanh-based MLP and initializes it to force saturation (e.g., very large weights), then verifies that [NeuralDbg](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#53-439) detects this.

```bash
python3 tests/repro_saturation.py
```

### Manual Verification
Reviewing the generated Mermaid graph to ensure the "Saturated Activation" events are correctly linked to subsequent "Vanishing Gradient" events.
