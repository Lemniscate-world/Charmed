# Walkthrough: Saturated Activation Detection

In this step-by-step example, I have implemented a new feature for [NeuralDbg](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#53-474): **Saturated Activation Detection**. This feature allows the engine to detect when bounded activation functions (like Tanh/Sigmoid) are operating in their saturated regimes.

## Changes Made

### NeuralDbg Causal Engine

In [neuraldbg.py](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py), I added logic to calculate the `saturation_ratio` in [_compute_activation_stats](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#187-212).

```python
# Calculate saturation ratio (for Sigmoid or Tanh typically)
# We consider a value saturated if it's very close to 1.0 or -1.0
saturation_ratio = (t_float.abs() > 0.95).float().mean().item()
```

I also updated [_detect_activation_shift](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#218-238) and implemented [_explain_saturated_activations](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#341-363) to generate causal hypotheses.

## Verification Results

I created a verification script [tests/repro_saturation.py](file:///home/kuro/Documents/NeuralDBG/tests/repro_saturation.py) that forces saturation in a Tanh-based MLP.

### Test Execution

The test captures events and verifies that a hypothesis is generated.

```bash
Captured events: 5
Event: EventType.ACTIVATION_REGIME_SHIFT in 2, Step 1
  Saturation Ratio: 1.0000
Event: EventType.ACTIVATION_REGIME_SHIFT in root, Step 1
  Saturation Ratio: 1.0000
...
Hypotheses for 'saturated_activations':
- Activation saturation detected in layer '2' at step 1 (Confidence: 1.00)
  Causal Chain: High saturation_ratio (1.00) in 2

Verification successful!
```

> [!NOTE]
> During development, I encountered and fixed a `NameError` in the engine and a `TypeError` in the test script, demonstrating the importance of iterative verification.

## Conclusion

The [NeuralDbg](file:///home/kuro/Documents/NeuralDBG/neuraldbg.py#53-474) engine is now capable of identifying saturated activations, which is a critical precursor to vanishing gradients in many architectures.
