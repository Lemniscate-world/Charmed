# Update Ollama Support for Cloud and Local

This plan addresses the user's request to support both local and cloud Ollama models. Currently, Void treats Ollama as a local-only provider without authentication. By adding an API Key field and passing it to the Ollama and OpenAI SDKs, users will be able to connect to remote/hosted Ollama instances.

## Proposed Changes

### Configuration Layer

#### [MODIFY] [modelCapabilities.ts](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/common/modelCapabilities.ts)
- Add `apiKey: ''` to `defaultProviderSettings.ollama`.

#### [MODIFY] [voidSettingsTypes.ts](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/common/voidSettingsTypes.ts)
- Update [displayInfoOfSettingName](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/common/voidSettingsTypes.ts#140-235) to include `apiKey` for [ollama](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#628-651).
- Add a specific placeholder for Ollama API keys.

---

### LLM Pipeline Layer

#### [MODIFY] [sendLLMMessage.impl.ts](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts)
- In [newOpenAICompatibleSDK](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#72-173), use the API Key from settings if provided for the [ollama](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#628-651) provider, falling back to `'noop'` if empty.
- Update [newOllamaSDK](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#620-627) to accept `apiKey` and include it in the `headers` as `Authorization: Bearer <key>`.
- Update [ollamaList](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#628-651) and [sendOllamaFIM](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#652-682) to retrieve and pass the `apiKey` to [newOllamaSDK](file:///home/kuro/Documents/Astral/src/vs/workbench/contrib/void/electron-main/llmMessage/sendLLMMessage.impl.ts#620-627).

## Verification Plan

### Automated Tests
- I will run the existing unit tests to ensure no regressions in basic provider logic.
- Command: `npm run test-node` (focused on LLM message logic if possible).

### Manual Verification
1. Open Void Settings.
2. Navigate to the Ollama provider.
3. **Verify** that an "API Key" field is now visible.
4. **Verify** that local Ollama still works (leave API Key blank, endpoint at `http://127.0.0.1:11434`).
5. **Instruction for User**: To test cloud support, enter a remote Ollama endpoint and its corresponding API key, then try refreshing the model list.
