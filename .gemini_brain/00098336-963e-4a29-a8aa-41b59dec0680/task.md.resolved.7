# Task: Cross-Editor Rule Synchronization & Repo Cleanup

- [x] Rules Generalization (from Sugar Wisdom)
- [/] Educational Session: The Probabilistic Transformer
    - [ ] Phase 1: Conceptual Briefings
        - [ ] Briefing 1: Synthetic Data Generation (The "Ground Truth")
        - [ ] Briefing 2: Transformer Encoder & Self-Attention
        - [ ] Briefing 3: Probabilistic Head & Gaussian Distribution
        - [ ] Briefing 4: Gaussian NLL Loss (Mathematical Objective)
    - [ ] Phase 2: Implementation (12 Steps)
        - [ ] 1. Create `synthetic_gen.py` (Sine + Gaussian Noise)
        - [ ] 2. Implement `TimeSeriesDataset` (Sliding Windows)
        - [ ] 3. Build `PositionalEncoding` module
        - [ ] 4. Build `TransformerEncoder` core
        - [ ] 5. Implement `GaussianProbHead` layer
        - [ ] 6. Integrate into `ProbabilisticTransformer` model
        - [ ] 7. Define `GaussianNLL` loss function
        - [ ] 8. Implement Training Loop (Optimization)
        - [ ] 9. Setup Validation Pipeline
        - [ ] 10. Run Baseline Training
        - [ ] 11. Visualize Mean and Standard Deviation
        - [ ] 12. Debug with NeuralDBG activation stats







