- [x] Rules Restoration & Merging (Continuity, Protocol, Multi-lingual)
- [/] Educational Session: The Probabilistic Transformer
    - [x] Briefing: The Normal Distribution (Loi Normale) & Probability Space
    - [x] Visualization: 3D Distribution Comparison (Gaussian vs Others)
    - [ ] Phase 1: Conceptual Briefings (Remaining)
        - [x] Briefing 1: Synthetic Data Generation (The "Ground Truth")
        - [x] Briefing 2: Transformer Encoder & Self-Attention
        - [ ] Briefing 3: Probabilistic Head & Gaussian Distribution
        - [ ] Briefing 4: Gaussian NLL Loss (Mathematical Objective)
    - [/] Phase 2: Implementation (12 Steps)
        - [x] 1. Create [synthetic_gen.py](file:///home/kuro/Documents/Aladin/src/synthetic_gen.py) (Sine + Gaussian Noise)
        - [x] 2. Implement [TimeSeriesDataset](file:///home/kuro/Documents/Aladin/src/dataset.py#5-27) (Sliding Windows)
        - [/] 3. Build `PositionalEncoding` module
        - [ ] 4. Build `TransformerEncoder` core

        - [ ] 5. Implement `GaussianProbHead` layer
        - [ ] 6. Integrate into `ProbabilisticTransformer` model
        - [ ] 7. Define `GaussianNLL` loss function
        - [ ] 8. Implement Training Loop (Optimization)
        - [ ] 9. Setup Validation Pipeline
        - [ ] 10. Run Baseline Training
        - [ ] 11. Visualize Mean and Standard Deviation
        - [ ] 12. Debug with NeuralDBG activation stats
