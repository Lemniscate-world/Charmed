# Brain MVP — Implementation Plan

A personal AI operating layer: **one chat interface** that reads, writes, and syncs across your tools (Linear, Obsidian, Telegram) using a **local LLM via Ollama** (no paid APIs).

## Quick Answer: What's a Product Spec?

A product spec is a formal document describing what to build, for whom, and why. **We don't need one for an MVP.** This implementation plan serves the same purpose — it defines what we're building, in what order, and how we'll verify it works. Ship first, document later.

---

## Tech Stack

| Layer | Choice | Why |
|---|---|---|
| **Language** | Python 3.12 | Already installed, great ecosystem for APIs/bots |
| **LLM** | Ollama (local) | Free, private, runs on your machine |
| **Model** | `mistral` or `llama3.2` | Good balance of quality vs. resource usage |
| **Database** | SQLite | Zero setup, conversation memory |
| **Linear API** | GraphQL (via `requests`) | Linear's official API |
| **Obsidian** | Direct filesystem (markdown) | Obsidian vaults are just folders of [.md](file:///home/kuro/Documents/Dissect/TESTS.md) files |
| **Chat UI** | Telegram Bot (Phase 1) | Free, instant, mobile-ready |

---

## Project Structure

```
/home/kuro/.gemini/antigravity/scratch/brain/
├── AI_GUIDELINES.md          # Adapted from Dissect
├── .cursorrules              # Adapted from Dissect
├── pyproject.toml
├── brain/
│   ├── __init__.py
│   ├── core/
│   │   ├── __init__.py
│   │   ├── engine.py         # Main Brain engine — routes messages to LLM + tools
│   │   ├── llm.py            # Ollama LLM wrapper
│   │   └── memory.py         # SQLite conversation memory
│   ├── connectors/
│   │   ├── __init__.py
│   │   ├── base.py           # Base connector interface
│   │   ├── linear.py         # Linear API connector
│   │   ├── obsidian.py       # Obsidian vault read/write
│   │   └── web.py            # Web search connector
│   ├── interfaces/
│   │   ├── __init__.py
│   │   ├── telegram_bot.py   # Telegram bot interface
│   │   └── cli.py            # Simple CLI for local testing
│   └── config.py             # Configuration (API keys, paths, etc.)
├── tests/
│   ├── test_llm.py
│   ├── test_memory.py
│   ├── test_linear.py
│   └── test_obsidian.py
└── README.md
```

---

## Proposed Changes

### Phase 1 — Project Setup

#### [NEW] [AI_GUIDELINES.md](file:///home/kuro/.gemini/antigravity/scratch/brain/AI_GUIDELINES.md)
Adapted from Dissect's guidelines. Core principles (SRP, KISS, YAGNI, critical thinking, traceability) kept. Dissect-specific refs (visualization, parsers) replaced with Brain-specific context (connectors, LLM, tools).

#### [NEW] [.cursorrules](file:///home/kuro/.gemini/antigravity/scratch/brain/.cursorrules)
Adapted from Dissect. Same quality bar, new project context.

#### [NEW] [pyproject.toml](file:///home/kuro/.gemini/antigravity/scratch/brain/pyproject.toml)
Python project config with dependencies: `requests`, `python-telegram-bot`, `ollama` (Python client).

#### System: Install Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral
```

> [!IMPORTANT]
> Ollama installation requires your approval — it's a system-level install. We'll also need ~4GB for the `mistral` model.

---

### Phase 2 — Core Brain Engine

#### [NEW] [engine.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/engine.py)
The central orchestrator. Takes a user message, checks conversation history, builds a prompt with available tools, sends to Ollama, parses tool calls, executes them, returns the response. Simple loop: `message → LLM → [tool calls] → response`.

#### [NEW] [llm.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/llm.py)
Thin wrapper around the Ollama Python client. Handles: model selection, streaming, tool/function calling format, error handling.

#### [NEW] [memory.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/memory.py)
SQLite-based conversation memory. Stores messages with timestamps. Retrieves recent context for the LLM. Simple schema: `conversations(id, created_at)`, `messages(id, conversation_id, role, content, timestamp)`.

---

### Phase 3 — Connectors

#### [NEW] [base.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/base.py)
Abstract base class: `name`, `description`, `available_actions()`, `execute(action, params)`.

#### [NEW] [linear.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/linear.py)
Linear GraphQL API connector. Actions: `list_issues`, `create_issue`, `update_issue`, `list_projects`, `search_issues`. Requires `LINEAR_API_KEY` env var.

#### [NEW] [obsidian.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/obsidian.py)
Reads/writes/searches markdown files in a configured vault path. Actions: `search_notes`, `read_note`, `create_note`, `append_to_note`. Requires `OBSIDIAN_VAULT_PATH` env var.

#### [NEW] [web.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/web.py)
Basic web search using DuckDuckGo (free, no API key). Actions: `search_web`, `read_url`.

---

### Phase 4 — Chat Interface

#### [NEW] [cli.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/interfaces/cli.py)
Simple terminal REPL for testing locally. `python -m brain.interfaces.cli` → interactive chat.

#### [NEW] [telegram_bot.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/interfaces/telegram_bot.py)
Telegram bot using `python-telegram-bot`. Receives messages, forwards to Brain engine, sends responses back. Requires `TELEGRAM_BOT_TOKEN` env var.

---

## Verification Plan

### Automated Tests
```bash
# Run all tests
python -m pytest tests/ -v

# Run specific connector test
python -m pytest tests/test_obsidian.py -v
```

Tests will cover:
- **LLM wrapper**: Mock Ollama responses, verify prompt construction
- **Memory**: SQLite read/write, conversation retrieval
- **Linear connector**: Mock API responses, verify GraphQL queries
- **Obsidian connector**: Create temp vault, test read/search/write

### Manual Verification
1. **CLI chat test**: Run `python -m brain.interfaces.cli`, type a question, verify LLM responds
2. **Obsidian test**: Ask the brain to search/create a note, verify the file appears in the vault
3. **Linear test**: Ask the brain to list issues, verify it returns real data (requires API key)
4. **Telegram test**: Send a message to the bot on Telegram, verify response (requires bot token)

> [!NOTE]
> For Linear and Telegram, you'll need to provide your API keys. I'll prompt you when we reach that phase.
