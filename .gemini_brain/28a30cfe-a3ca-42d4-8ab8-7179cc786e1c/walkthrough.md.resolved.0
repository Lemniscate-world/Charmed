# Brain MVP — Walkthrough

## What Was Built

A personal AI operating layer: **one chat interface** that connects Linear, Obsidian, and web search through a local LLM (Ollama). The entire project was scaffolded from scratch with AI guidelines adapted from the [Dissect](file:///home/kuro/Documents/Dissect) project.

## Project Location

```
/home/kuro/.gemini/antigravity/scratch/brain/
```

> [!TIP]
> Set this as your active workspace to work on it further.

## Architecture

```mermaid
graph TD
    A[User] -->|types message| B[CLI / Telegram]
    B --> C[Brain Engine]
    C --> D[Ollama LLM]
    C --> E[SQLite Memory]
    C --> F[Connectors]
    F --> G[Linear API]
    F --> H[Obsidian Vault]
    F --> I[DuckDuckGo]
    D -->|tool calls| F
    F -->|results| D
    D -->|response| B
```

## Files Created

| File | Purpose |
|---|---|
| [AI_GUIDELINES.md](file:///home/kuro/.gemini/antigravity/scratch/brain/AI_GUIDELINES.md) | Adapted from Dissect |
| [.cursorrules](file:///home/kuro/.gemini/antigravity/scratch/brain/.cursorrules) | Adapted from Dissect |
| [config.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/config.py) | Env-based configuration |
| [engine.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/engine.py) | Main orchestrator |
| [llm.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/llm.py) | Ollama wrapper |
| [memory.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/core/memory.py) | SQLite conversation storage |
| [linear.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/linear.py) | Linear GraphQL connector |
| [obsidian.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/obsidian.py) | Obsidian vault connector |
| [web.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/connectors/web.py) | DuckDuckGo search connector |
| [cli.py](file:///home/kuro/.gemini/antigravity/scratch/brain/brain/interfaces/cli.py) | Interactive terminal REPL |

## Test Results

**33/33 tests passing** ✅

```
tests/test_linear.py    — 7 passed (mocked GraphQL)
tests/test_llm.py       — 7 passed (tool parsing, errors)
tests/test_memory.py    — 7 passed (SQLite CRUD)
tests/test_obsidian.py  — 12 passed (filesystem ops)
```

## How to Run

### 1. Install Ollama (one-time)
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral
```

### 2. Configure
```bash
cd /home/kuro/.gemini/antigravity/scratch/brain
cp .env.example .env
# Edit .env → set OBSIDIAN_VAULT_PATH, LINEAR_API_KEY (optional)
```

### 3. Start Brain
```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Run Brain
source .venv/bin/activate
python -m brain.interfaces.cli
```

## What's Next

- [ ] **Install Ollama** → `sudo apt install curl && curl -fsSL https://ollama.com/install.sh | sh`
- [ ] **Set Obsidian vault path** → edit [.env](file:///home/kuro/Documents/Dissect/.env)
- [ ] **Add Linear API key** → optional, from Linear Settings → API
- [ ] **Telegram bot** → create bot via @BotFather, add token to [.env](file:///home/kuro/Documents/Dissect/.env)
- [ ] **Phase 2 intelligence** → auto-syncing across tools, suggested updates
